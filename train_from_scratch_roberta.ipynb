{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "radio-router",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "\n",
    "file_total_original = \"data/hate_speech_hindi_english.tsv\"\n",
    "df_total = pd.read_csv(file_total_original,sep=\"\\t\",encoding='utf-8',quoting=csv.QUOTE_NONE,usecols=[0,1])\n",
    "df_total = df_total.dropna()\n",
    "\n",
    "total_sentences = list(df_total['text'].values)\n",
    "total_labels = list(df_total['label'].values)\n",
    "\n",
    "fin = open(\"sentences.txt\",\"w+\",encoding=\"utf-8\")\n",
    "\n",
    "for line in total_sentences:\n",
    "    fin.write(line+\"\\n\")\n",
    "\n",
    "length = []\n",
    "for line in total_sentences:\n",
    "    line_list = line.split()\n",
    "    length.append(len(line_list))\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "#print(Counter(length).most_common())\n",
    "\n",
    "max_length = 2*Counter(length).most_common()[-1][0]\n",
    "print(max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "engaged-booth",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "\n",
    "tokenizer.train(files=\"sentences.txt\", vocab_size=52_000, min_frequency=2, special_tokens=[\n",
    "    \"<s>\",\n",
    "    \"<pad>\",\n",
    "    \"</s>\",\n",
    "    \"<unk>\",\n",
    "    \"<mask>\",\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "widespread-rebound",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CM_bert\\\\vocab.json', 'CM_bert\\\\merges.txt']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!mkdir CM_bert\n",
    "tokenizer.save_model(\"CM_bert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "endangered-class",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.implementations import ByteLevelBPETokenizer\n",
    "from tokenizers.processors import BertProcessing\n",
    "\n",
    "tokenizer = ByteLevelBPETokenizer(\n",
    "    \"./CM_bert/vocab.json\",\n",
    "    \"./CM_bert/merges.txt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "increasing-parliament",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer._tokenizer.post_processor = BertProcessing(\n",
    "    (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "    (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    ")\n",
    "tokenizer.enable_truncation(max_length=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "governing-cable",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>',\n",
       " 'K',\n",
       " 'now',\n",
       " 'ing',\n",
       " 'Ä ki',\n",
       " 'Ä Vikas',\n",
       " 'Ä kitna',\n",
       " 'Ä samjha',\n",
       " 'ta',\n",
       " 'Ä hai',\n",
       " 'Ä P',\n",
       " 'riyanka',\n",
       " 'Ä aur',\n",
       " 'Ä It',\n",
       " 'ch',\n",
       " 'Ä Gu',\n",
       " 'ard',\n",
       " 'Ä Luv',\n",
       " 'Ä ko',\n",
       " ',',\n",
       " 'Ä usne',\n",
       " 'Ä bola',\n",
       " 'Ä tha',\n",
       " 'Ä B',\n",
       " 'en',\n",
       " 'Ä wali',\n",
       " 'Ä baat',\n",
       " 'Ä me',\n",
       " 'Ä ab',\n",
       " 'Ä Sallu',\n",
       " 'Ä ne',\n",
       " 'Ä bhi',\n",
       " 'Ä agree',\n",
       " 'Ä kiya',\n",
       " '!',\n",
       " '</s>']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(total_sentences[0]).tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cardiac-universe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "stable-interface",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaConfig\n",
    "\n",
    "config = RobertaConfig(\n",
    "    vocab_size=52_000,\n",
    "    max_position_embeddings=max_length,\n",
    "    num_attention_heads=3,\n",
    "    num_hidden_layers=3,\n",
    "    type_vocab_size=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "chinese-begin",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizerFast\n",
    "\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"./CM_bert\", max_len=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "recreational-gravity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61948960"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import RobertaForMaskedLM\n",
    "\n",
    "model = RobertaForMaskedLM(config=config)\n",
    "\n",
    "model.num_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dominant-identity",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\suman\\Miniconda3\\envs\\py3_env\\lib\\site-packages\\transformers\\data\\datasets\\language_modeling.py:128: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/language-modeling/run_mlm.py\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "from transformers import LineByLineTextDataset\n",
    "\n",
    "dataset = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"sentences.txt\",\n",
    "    block_size=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "subjective-ecology",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "received-dance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='27468' max='27468' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [27468/27468 12:01, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.613300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.605000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.760900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.490300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.600800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.554300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.644800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.679100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.561300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.708500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.385700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.487500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.565000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.606000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.613700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.745200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.421000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.375500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.557000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.626500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.683400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.571400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.594900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.573400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.425300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.439900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.482500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.700500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>0.750400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.455200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>0.586300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.346500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>0.450700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.492800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>0.473100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.425100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>0.581900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.440800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>0.565600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.374500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20500</td>\n",
       "      <td>0.378000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.672400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21500</td>\n",
       "      <td>0.710600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.509700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>0.514400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.443400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23500</td>\n",
       "      <td>0.631200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.557400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24500</td>\n",
       "      <td>0.570500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.570900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25500</td>\n",
       "      <td>0.573400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.408600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26500</td>\n",
       "      <td>0.578200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.653300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=27468, training_loss=0.5511127014737888, metrics={'train_runtime': 722.1708, 'train_samples_per_second': 38.035, 'total_flos': 81483449454720, 'epoch': 6.0})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./CM_bert\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=6,\n",
    "    per_device_train_batch_size=1,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    "    prediction_loss_only=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "seventh-wagon",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./CM_bert\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3_env",
   "language": "python",
   "name": "py3_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
