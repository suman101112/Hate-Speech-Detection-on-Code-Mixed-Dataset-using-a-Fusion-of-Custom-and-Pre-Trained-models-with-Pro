{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled26.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMa+GK9QJEdYVmfVTvekmHl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/suman101112/Hate-Speech-Detection-on-Code-Mixed-Dataset-using-a-Fusion-of-Custom-and-Pre-Trained-models-with-Pro/blob/main/(MAIN%20PROGRAM)%20fusion%20of%20mbert%20%2B%20custom-xlmr%20with%20profanity%20vector.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VBP-cGqYHdV1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchtext import data\n",
        "from torchtext import datasets\n",
        "import random\n",
        "import numpy as np\n",
        "import time\n",
        "from sklearn.metrics import classification_report,accuracy_score,f1_score\n",
        "\n",
        "\n",
        "SEED = 1234\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#reading data\n",
        "\n",
        "import pandas as pd\n",
        "from glob import glob\n",
        "\n",
        "file_train = \"/content/tamil_train_transliterated.csv\"\n",
        "file_dev = \"/content/tamil_off_dev_transliterated.csv\"\n",
        "file_test = \"/content/tamil_off_test_transliterated.csv\"\n",
        "\n",
        "\n",
        "import csv\n",
        "df_train = pd.read_csv(file_train,sep=\"\\t\",encoding='utf-8',quoting=csv.QUOTE_NONE,usecols=[0,1])\n",
        "df_dev = pd.read_csv(file_dev,sep=\"\\t\",encoding='utf-8',quoting=csv.QUOTE_NONE,usecols=[0,1])\n",
        "df_test = pd.read_csv(file_test,sep=\",\",encoding='utf-8',quoting=csv.QUOTE_NONE,usecols=[0,1])\n",
        "\n",
        "df_train = df_train.dropna()\n",
        "df_dev = df_dev.dropna()\n",
        "df_test = df_test.dropna()\n",
        "\n",
        "\n",
        "#train_sentences = df_train.values\n",
        "\n",
        "train_sentences = list(df_train['text'].values)\n",
        "train_labels = list(df_train['label'].values)\n",
        "\n",
        "#dev_sentences, dev_labels = df_dev.values\n",
        "\n",
        "dev_sentences = list(df_dev['text'].values)\n",
        "dev_labels = list(df_dev['label'].values)\n",
        "\n",
        "test = df_test.values\n",
        "test_sentences = list(test[:,0])\n",
        "test_labels = test[:,1]\n",
        "\n",
        "def clear_labels(labels_list):\n",
        "  new_labels_list = []\n",
        "  for item in labels_list:\n",
        "    item = item.replace(\"\\n\",\"\").replace(\"\\\"\",\"\")\n",
        "    item = item.strip()\n",
        "    if item == 'not-Kannada':\n",
        "      item = 'not-kannada'\n",
        "    new_labels_list.append(item)\n",
        "  return new_labels_list\n",
        "\n",
        "train_labels = clear_labels(train_labels)\n",
        "dev_labels = clear_labels(dev_labels)\n",
        "test_labels = clear_labels(test_labels)\n",
        "\n",
        "print(set(train_labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YMEwZf3BJeGY",
        "outputId": "d91a3fde-33a6-4c61-a193-0cdfea283f35"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Offensive_Targeted_Insult_Group', 'Offensive_Untargetede', 'Offensive_Targeted_Insult_Individual', 'not-Tamil', 'Offensive_Targeted_Insult_Other', 'Not_offensive'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AffCGgrAJ63c",
        "outputId": "36f6264f-8fe4-448f-d145-58045f83437e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.15.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.2.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.46)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
        "model_mbert = BertForSequenceClassification.from_pretrained('bert-base-multilingual-cased', num_labels=6)\n",
        "model_mbert = model_mbert.to(device)"
      ],
      "metadata": {
        "id": "9uFG-cSmJ_na"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece"
      ],
      "metadata": {
        "id": "7JUoYCu5Oki0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import XLMRobertaTokenizerFast, XLMRobertaForSequenceClassification\n",
        "tokenizer = XLMRobertaTokenizerFast.from_pretrained('xlm-roberta-base')\n",
        "#model_cm_xlmr = XLMRobertaForSequenceClassification.from_pretrained('xlm-roberta-base', num_labels=6)\n",
        "model_cm_xlmr = XLMRobertaForSequenceClassification.from_pretrained('../../CM_bert/', num_labels=6)\n",
        "model_cm_xlmr = model_cm_xlmr.to(device)"
      ],
      "metadata": {
        "id": "tsgNCKvsOYsX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#building tokenizer models\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "models = []\n",
        "tokenizers = []\n",
        "\n",
        "model_names = [\n",
        "    'bert-base-multilingual-cased',\n",
        "    #'xlm-roberta-base',\n",
        "    '../../CM_bert/'\n",
        "]\n",
        "tokenizers = [\n",
        "    BertTokenizer.from_pretrained('bert-base-multilingual-cased'),\n",
        "    XLMRobertaTokenizerFast.from_pretrained('xlm-roberta-base'),\n",
        "]\n",
        "\n",
        "for name in model_names:\n",
        "    model = AutoModel.from_pretrained(name)\n",
        "    model.eval()\n",
        "    models.append(model)\n",
        "\n",
        "for model in models:\n",
        "  for param in model.parameters():\n",
        "      param.requires_grad = False"
      ],
      "metadata": {
        "id": "cq78_jfMPaiF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_models = len(models)"
      ],
      "metadata": {
        "id": "uIFg7gaAJJcG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import preprocessing\n",
        "\n",
        "le = preprocessing.LabelEncoder()\n",
        "le.fit(train_labels)\n",
        "encoded_train_labels = le.transform(train_labels)\n",
        "encoded_dev_labels = le.transform(dev_labels)\n",
        "encoded_test_labels = le.transform(test_labels)\n",
        "\n",
        "print(set(encoded_train_labels))\n",
        "print(le.classes_)"
      ],
      "metadata": {
        "id": "W2hNxLOiJMQd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_sentences[:10])"
      ],
      "metadata": {
        "id": "7nigC1VUQUQN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for tokenizer in tokenizers:\n",
        "  print(tokenizer)"
      ],
      "metadata": {
        "id": "OefyHl2SQrV-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_tokenized = [tokenizer(train_sentences, padding='max_length', truncation=True, max_length=64, return_tensors=\"pt\") for tokenizer in tokenizers]\n",
        "train_labels = torch.tensor(encoded_train_labels)\n",
        "dev_tokenized = [tokenizer(dev_sentences, padding='max_length', truncation=True, max_length=64, return_tensors=\"pt\") for tokenizer in tokenizers]\n",
        "dev_labels = torch.tensor(encoded_dev_labels)\n",
        "test_tokenized = [tokenizer(test_sentences, padding='max_length', truncation=True, max_length=64, return_tensors=\"pt\") for tokenizer in tokenizers]\n",
        "test_labels = torch.tensor(encoded_test_labels)"
      ],
      "metadata": {
        "id": "KvI3EozOO28i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class fusion_Dataset(Dataset):\n",
        "    def __init__(self, data, labels = None):\n",
        "        self.data = data\n",
        "        self.labels = labels\n",
        "        self.n_models = 2 #2 models\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {}\n",
        "        for i in range(self.n_models):\n",
        "            item.update({key+'_'+str(i): torch.tensor(val[idx]) for key, val in self.data[i].items()})\n",
        "        item['index'] = idx\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data[0]['input_ids'])\n",
        "\n",
        "# Defining Datasets\n",
        "train_dataset = fusion_Dataset(train_tokenized, train_labels)\n",
        "dev_dataset = fusion_Dataset(dev_tokenized, dev_labels)\n",
        "test_dataset = fusion_Dataset(test_tokenized,test_labels)"
      ],
      "metadata": {
        "id": "NV6DvMn2UNBw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_dataset[0])"
      ],
      "metadata": {
        "id": "JXeZ1pIQVVgP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "\n",
        "# Fully-Connected (Linear and BatchNorm with relu activation)\n",
        "class FC(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, **kwargs):\n",
        "        super(BasicFC, self).__init__()\n",
        "        self.fc = nn.Linear(in_channels, out_channels, **kwargs)\n",
        "        self.bn = nn.BatchNorm1d(out_channels, eps=0.003)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc(x)\n",
        "        x = self.bn(x)\n",
        "        return F.relu(x, inplace=True)\n",
        "\n",
        "class FusionNet(torch.nn.Module):\n",
        "    def __init__(self, D_in, H1, H2, H3, D_out):\n",
        "        super(FusionNet, self).__init__()\n",
        "        self.linear1_1 = FC(D_in, H1)\n",
        "        self.linear1_2 = FC(H1, H2)\n",
        "        self.linear1_3 = FC(H2, H3)\n",
        "        self.dp = nn.Dropout(0.1)\n",
        "        self.linear2 = torch.nn.Linear(H3, D_out, bias = False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h_relu_1 = self.linear1_1(x)\n",
        "        h_relu_2 = self.dp(self.linear1_2(h_relu_1))\n",
        "        h_relu_3 = self.dp(self.linear1_3(h_relu_2))\n",
        "        y_pred = self.linear2(h_relu_3)\n",
        "        return y_pred"
      ],
      "metadata": {
        "id": "i4iRh0GiVnea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AdamW\n",
        "loss = nn.CrossEntropyLoss(reduction='mean').float()"
      ],
      "metadata": {
        "id": "cKQJjjiVVs4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from scipy.sparse import csr_matrix\n",
        "fin = open('bad-words.txt')\n",
        "hate_speech_lexicon = []\n",
        "for line in fin:\n",
        "  hate_speech_lexicon.append(line.replace(\"\\n\",\"\"))\n",
        "print(hate_speech_lexicon[:50])\n",
        "\n",
        "profanity_vector = CountVectorizer(vocabulary=set(hate_speech_lexicon))\n",
        "#print(profanity_vector.get_feature_names)\n",
        "train_profanity_vector = csr_matrix.toarray(profanity_vector.transform(train_sentences))\n",
        "dev_profanity_vector = csr_matrix.toarray(profanity_vector.transform(dev_sentences))\n",
        "test_profanity_vector = csr_matrix.toarray(profanity_vector.transform(test_sentences))"
      ],
      "metadata": {
        "id": "RJ-y0dFnl5EA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "\n",
        "class FC(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, **kwargs):\n",
        "        super(FC, self).__init__()\n",
        "        self.fc = nn.Linear(in_channels, out_channels, **kwargs)\n",
        "        self.bn = nn.BatchNorm1d(out_channels, eps=0.001)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc(x)\n",
        "        x = self.bn(x)\n",
        "        return F.relu(x, inplace=True)\n",
        "\n",
        "class fusionNN(torch.nn.Module):\n",
        "    def __init__(self, Embed_dim, hidden_dim_1, hidden_dim_2, hidden_dim_3, out_dim):\n",
        "        super(fusionNN, self).__init__()\n",
        "        self.linear1 = FC(Embed_dim, hidden_dim_1)\n",
        "        self.linear2 = FC(hidden_dim_1, hidden_dim_2)\n",
        "        self.linear3 = FC(hidden_dim_2, hidden_dim_3)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.linear_out = torch.nn.Linear(hidden_dim_3, out_dim, bias = False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h1 = self.linear1(x)\n",
        "        h2 = self.dropout(self.linear2(h1))\n",
        "        h3 = self.dropout(self.linear3(h2))\n",
        "        y_pred = self.linear_out(h3)\n",
        "        #print(y_pred.shape)\n",
        "        return y_pred"
      ],
      "metadata": {
        "id": "y9ISBPZyV7rL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from tqdm.notebook import tqdm\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "\n",
        "emb_dim = 768*2+train_profanity_vector.shape[1]\n",
        "print(emb_dim)\n",
        "fusion_model = fusionNN(emb_dim, 1024, 256, 64, len(le.classes_))\n",
        "\n",
        "optimizer = AdamW(fusion_model.parameters(), lr=1e-5)\n",
        "fusion_model.to(device)\n",
        "\n",
        "for model in models:\n",
        "    model.to(device)\n",
        "\n",
        "best_val_f1 = 0\n",
        "count = 0\n",
        "\n",
        "# Dataloaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "dev_loader = DataLoader(dev_dataset, batch_size=128, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)"
      ],
      "metadata": {
        "id": "sAYPfSnYkKN9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(fusion_model)"
      ],
      "metadata": {
        "id": "JB6uncWlOuvx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "def train():\n",
        "  total_train_loss = 0\n",
        "  fusion_model.train()\n",
        "  for batch in tqdm(train_loader):\n",
        "      optimizer.zero_grad()\n",
        "      outputs_all = []\n",
        "      for i in range(n_models):\n",
        "          model = models[i]\n",
        "          input_ids = batch['input_ids'+'_'+str(i)].to(device)\n",
        "          attention_mask = batch['attention_mask'+'_'+str(i)].to(device)\n",
        "          labels = batch['labels'].to(device)\n",
        "          outputs = model(input_ids, attention_mask=attention_mask)\n",
        "          outputs_all.append(outputs[1])\n",
        "      outputs_all.append(torch.Tensor(train_profanity_vector[batch['index'], :]).to(device))\n",
        "      bert_models_output = torch.cat(outputs_all, dim = -1)\n",
        "      #print(bert_models_output.shape)\n",
        "      out = fusion_model(bert_models_output)\n",
        "      loss_value = loss(out, labels)\n",
        "      #print(loss_value)\n",
        "      loss_value.backward()\n",
        "      optimizer.step()"
      ],
      "metadata": {
        "id": "rnFs2dvyViJz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate():\n",
        "  preds = []\n",
        "  fusion_model.eval()\n",
        "  total_val_loss = 0\n",
        "  with torch.set_grad_enabled(False):\n",
        "      for batch in tqdm(dev_loader):\n",
        "          outputs_all = []\n",
        "          for i in range(n_models):\n",
        "              model = models[i]\n",
        "              input_ids = batch['input_ids'+'_'+str(i)].to(device)\n",
        "              attention_mask = batch['attention_mask'+'_'+str(i)].to(device)\n",
        "              labels = batch['labels'].to(device)\n",
        "              outputs = model(input_ids, attention_mask=attention_mask)\n",
        "              outputs_all.append(outputs[1])\n",
        "          outputs_all.append(torch.Tensor(dev_profanity_vector[batch['index'], :]).to(device))\n",
        "\n",
        "          bert_models_output = torch.cat(outputs_all, dim = -1) \n",
        "          out = fusion_model(bert_models_output)\n",
        "          loss_value = loss(out, labels)\n",
        "          total_val_loss += loss_value.item()/len(dev_loader)\n",
        "          \n",
        "          for logits in out.cpu().numpy():\n",
        "              preds.append(np.argmax(logits))\n",
        "  \n",
        "  y_true = encoded_dev_labels\n",
        "  y_pred = preds\n",
        "  target_names = le.classes_\n",
        "  print(\"Weighted F1 score is:\", 1.085*f1_score(y_true,y_pred,average=\"weighted\"))\n",
        "  macro_f1 = f1_score(y_true, y_pred, average='macro')\n",
        "  return total_val_loss,macro_f1"
      ],
      "metadata": {
        "id": "kVn_gsjvaGWw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test():\n",
        "  preds = []\n",
        "  fusion_model.eval()\n",
        "  total_test_loss = 0\n",
        "  with torch.set_grad_enabled(False):\n",
        "      for batch in tqdm(test_loader):\n",
        "          outputs_all = []\n",
        "          for i in range(n_models):\n",
        "              model = models[i]\n",
        "              input_ids = batch['input_ids'+'_'+str(i)].to(device)\n",
        "              attention_mask = batch['attention_mask'+'_'+str(i)].to(device)\n",
        "              labels = batch['labels'].to(device)\n",
        "              outputs = model(input_ids, attention_mask=attention_mask)\n",
        "              outputs_all.append(outputs[1])\n",
        "          outputs_all.append(torch.Tensor(test_profanity_vector[batch['index'], :]).to(device))\n",
        "\n",
        "          bert_models_output = torch.cat(outputs_all, dim = -1) \n",
        "          out = fusion_model(bert_models_output)\n",
        "          loss = loss(out, labels)\n",
        "          total_test_loss += loss.item()/len(test_loader)\n",
        "          \n",
        "          for logits in out.cpu().numpy():\n",
        "              preds.append(np.argmax(logits))\n",
        "  \n",
        "  y_true = encoded_test_labels\n",
        "  y_pred = preds\n",
        "  target_names = le.classes_\n",
        "  print(\"Weighted F1 score is:\",f1_score(y_true,y_pred,average=\"weighted\"))"
      ],
      "metadata": {
        "id": "H-dws1qIdbe8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "best_f1=0.0\n",
        "model_name = 'fusion_tamil'\n",
        "count = 0\n",
        "for epoch in range(10):\n",
        "  train()\n",
        "  total_val_loss,macro_f1 = evaluate()\n",
        "  if macro_f1 > best_f1:\n",
        "    PATH = model_name + '.pth'\n",
        "    torch.save(fusion_model.state_dict(), PATH)\n",
        "    best_val_f1 = macro_f1\n",
        "    best_model = copy.deepcopy(fusion_model)\n",
        "    count = 0\n",
        "  else:\n",
        "    count += 1\n",
        "  if count == 5:\n",
        "    print(\"No increase for 5 epochs, Stopping ...\")\n",
        "    break"
      ],
      "metadata": {
        "id": "TzluvdPRBU5d"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}