{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "muslim-night",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device='cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "interpreted-effectiveness",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.metrics import classification_report,accuracy_score,f1_score\n",
    "\n",
    "total_t0 = time.time()\n",
    "\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "banner-tragedy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "\n",
    "file_total_original = \"data/hate_speech_hindi_english.tsv\"\n",
    "df_total = pd.read_csv(file_total_original,sep=\"\\t\",encoding='utf-8',quoting=csv.QUOTE_NONE,usecols=[0,1])\n",
    "df_total = df_total.dropna()\n",
    "\n",
    "total_sentences = list(df_total['text'].values)\n",
    "total_labels = list(df_total['label'].values)\n",
    "\n",
    "fin = open(\"sentences.txt\",\"w+\",encoding=\"utf-8\")\n",
    "\n",
    "for line in total_sentences:\n",
    "    fin.write(line+\"\\n\")\n",
    "\n",
    "length = []\n",
    "for line in total_sentences:\n",
    "    line_list = line.split()\n",
    "    length.append(len(line_list))\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "#print(Counter(length).most_common())\n",
    "\n",
    "max_length = 2*Counter(length).most_common()[-1][0]\n",
    "print(max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "retired-nomination",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "new_total_labels=[]\n",
    "for item in total_labels:\n",
    "    if 'n' in item:\n",
    "        new_total_labels.append('no')\n",
    "    else:\n",
    "        new_total_labels.append('yes')\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(new_total_labels)\n",
    "encoded_labels = le.transform(new_total_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "seasonal-shakespeare",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3434 858 286\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(zip(total_sentences,encoded_labels),columns=['text','label'])\n",
    "\n",
    "train = df.sample(frac = 0.75)\n",
    "test_valid = df.drop(train.index)\n",
    "\n",
    "test = test_valid.sample(frac=0.75)\n",
    "valid = test_valid.drop(test.index)\n",
    "\n",
    "print(len(train),len(test),len(valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "sought-portable",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_generator(tokenizer,sentences,labels):\n",
    "    \n",
    "    sent_index = []\n",
    "    input_ids = []\n",
    "    attention_masks =[]\n",
    "\n",
    "    for index,sent in enumerate(sentences):\n",
    "        \n",
    "        sent_index.append(index)\n",
    "        \n",
    "        encoded_dict = tokenizer.encode_plus(sent,\n",
    "                                             add_special_tokens=True,\n",
    "                                             max_length=max_length,\n",
    "                                             pad_to_max_length=True,\n",
    "                                             truncation = True,\n",
    "                                             return_attention_mask=True,\n",
    "                                             return_tensors='pt')\n",
    "        #print(encoded_dict['input_ids'].shape)\n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "\n",
    "        #print(encoded_dict['attention_mask'].shape)\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "    input_ids = torch.cat(input_ids,dim=0)\n",
    "    print(input_ids.shape)\n",
    "    attention_masks = torch.cat(attention_masks,dim=0)\n",
    "    print(attention_masks.shape)\n",
    "    labels = torch.tensor(labels)\n",
    "    print(labels.shape)\n",
    "    sent_index = torch.tensor(sent_index)\n",
    "\n",
    "    return sent_index,input_ids,attention_masks,labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "permanent-lindsay",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\suman\\Miniconda3\\envs\\py3_env\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2143: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3434, 134])\n",
      "torch.Size([3434, 134])\n",
      "torch.Size([3434])\n",
      "torch.Size([858, 134])\n",
      "torch.Size([858, 134])\n",
      "torch.Size([858])\n",
      "torch.Size([286, 134])\n",
      "torch.Size([286, 134])\n",
      "torch.Size([286])\n",
      "Original:  Knowing ki Vikas kitna samjhata hai Priyanka aur Itch Guard Luv ko, usne bola tha Ben wali baat me ab Sallu ne bhi agree kiya!\n",
      "Token IDs: tensor([    0,  1252,   513,   289,   329,   291,   453,   448,   513,   292,\n",
      "        10894,   292,   715,   587,   276,  1217,  1816,  5753,   293,   324,\n",
      "          565,   374,  1058,   276,  6560,   289,  2200,   521,   276,     2,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1])\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizerFast\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"./CM_bert\", max_len=max_length)\n",
    "\n",
    "train_sent_index,train_input_ids,train_attention_masks,train_encoded_label_tensors = encoder_generator(tokenizer,train['text'].values,train['label'].values)\n",
    "test_sent_index,test_input_ids,test_attention_masks,test_encoded_label_tensors = encoder_generator(tokenizer,test['text'].values,test['label'].values)\n",
    "valid_sent_index,valid_input_ids,valid_attention_masks,valid_encoded_label_tensors = encoder_generator(tokenizer,valid['text'].values,valid['label'].values)\n",
    "\n",
    "print('Original: ', total_sentences[0])\n",
    "print('Token IDs:', train_input_ids[0])\n",
    "#print(encoded_label_tensors)\n",
    "#print(encoded_test_label_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "minor-disposition",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3434, 134])\n",
      "torch.Size([3434, 134])\n",
      "torch.Size([3434])\n",
      "torch.Size([858, 134])\n",
      "torch.Size([858, 134])\n",
      "torch.Size([858])\n",
      "torch.Size([286, 134])\n",
      "torch.Size([286, 134])\n",
      "torch.Size([286])\n",
      "Original:  Knowing ki Vikas kitna samjhata hai Priyanka aur Itch Guard Luv ko, usne bola tha Ben wali baat me ab Sallu ne bhi agree kiya!\n",
      "Token IDs: tensor([  101, 12541, 10139, 10237, 10730, 11520, 82523, 10879, 10679, 12796,\n",
      "        10139, 10237, 10879, 28335, 59404, 10879, 11519, 82612, 10113, 13080,\n",
      "        10730, 11107, 10124, 10440, 29694, 10237, 25085, 10104, 10120, 10679,\n",
      "        11023, 41193, 10113, 13080, 97715, 10730, 29694, 10921, 26506, 10113,\n",
      "        13080,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0])\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "pt_train_sent_index,pt_train_input_ids,pt_train_attention_masks,pt_train_encoded_label_tensors = encoder_generator(bert_tokenizer,train['text'].values,train['label'].values)\n",
    "pt_test_sent_index,pt_test_input_ids,pt_test_attention_masks,pt_test_encoded_label_tensors = encoder_generator(bert_tokenizer,test['text'].values,test['label'].values)\n",
    "pt_valid_sent_index,pt_valid_input_ids,pt_valid_attention_masks,pt_valid_encoded_label_tensors = encoder_generator(bert_tokenizer,valid['text'].values,valid['label'].values)\n",
    "\n",
    "print('Original: ', total_sentences[0])\n",
    "print('Token IDs:', pt_train_input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "simplified-slovakia",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset,random_split\n",
    "\n",
    "train_dataset = TensorDataset(train_input_ids,train_attention_masks,train_encoded_label_tensors)\n",
    "test_dataset = TensorDataset(test_input_ids,test_attention_masks,test_encoded_label_tensors)\n",
    "valid_dataset = TensorDataset(valid_input_ids,valid_attention_masks,valid_encoded_label_tensors)\n",
    "\n",
    "pt_train_dataset = TensorDataset(pt_train_input_ids,pt_train_attention_masks,pt_train_encoded_label_tensors)\n",
    "pt_test_dataset = TensorDataset(pt_test_input_ids,pt_test_attention_masks,pt_test_encoded_label_tensors)\n",
    "pt_valid_dataset = TensorDataset(pt_valid_input_ids,pt_valid_attention_masks,pt_valid_encoded_label_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "assured-plaza",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader,SequentialSampler\n",
    "\n",
    "bs=32\n",
    "\n",
    "train_data_loader = DataLoader(train_dataset,\n",
    "                              sampler=SequentialSampler(train_dataset),\n",
    "                              batch_size=bs)\n",
    "valid_data_loader = DataLoader(valid_dataset,\n",
    "                              sampler=SequentialSampler(valid_dataset),\n",
    "                              batch_size=bs)\n",
    "test_data_loader = DataLoader(test_dataset,\n",
    "                            sampler=SequentialSampler(test_dataset),\n",
    "                            batch_size=bs)\n",
    "\n",
    "pt_train_data_loader = DataLoader(pt_train_dataset,\n",
    "                              sampler=SequentialSampler(pt_train_dataset),\n",
    "                              batch_size=bs)\n",
    "pt_valid_data_loader = DataLoader(pt_valid_dataset,\n",
    "                              sampler=SequentialSampler(pt_valid_dataset),\n",
    "                              batch_size=bs)\n",
    "pt_test_data_loader = DataLoader(pt_test_dataset,\n",
    "                            sampler=SequentialSampler(pt_test_dataset),\n",
    "                            batch_size=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "genuine-cradle",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./CM_bert were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ./CM_bert and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaForSequenceClassification, AdamW\n",
    "\n",
    "cm_model = RobertaForSequenceClassification.from_pretrained('./CM_bert',\n",
    "                                                     num_labels=len(le.classes_),\n",
    "                                                     output_attentions=False,\n",
    "                                                     output_hidden_states=False)\n",
    "#cm_model.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "enclosed-clearance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adcf902c58b24e0f889bda8a4d3ef52d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/625 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33f6c7d37d9a455ab031474d997ed21b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/672M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "pt_bert_model = BertForSequenceClassification.from_pretrained('bert-base-multilingual-uncased',\n",
    "                                                     num_labels=len(le.classes_),\n",
    "                                                     output_attentions=False,\n",
    "                                                     output_hidden_states=False)\n",
    "#pt_bert_model.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "sustained-briefing",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_parameters = list(cm_model.parameters()) + list(pt_bert_model.parameters())\n",
    "\n",
    "optimizer = AdamW(model_parameters,lr=2e-5,eps=1e-8)\n",
    "\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "epochs=10\n",
    "total_steps = len(train_data_loader) * epochs\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                           num_warmup_steps=0,\n",
    "                                           num_training_steps=total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cultural-calendar",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "    max_preds = preds.argmax(dim = 1, keepdim = True) # get the index of the max probability\n",
    "    correct = max_preds.squeeze(1).eq(y)\n",
    "    return correct.sum() / torch.FloatTensor([y.shape[0]]).to(device)\n",
    "\n",
    "def predictions_labels(preds,labels):\n",
    "    pred = np.argmax(preds,axis=1).flatten()\n",
    "    label = labels.flatten()\n",
    "    return pred,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "irish-yesterday",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "fc_linear = nn.Linear(2*len(le.classes_),len(le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "governing-adaptation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "alpha = 0.5\n",
    "\n",
    "def train(cm_model,cm_data_loader,pt_model,pt_data_loader):\n",
    "    total_train_loss = 0\n",
    "    total_train_acc = 0\n",
    "    \n",
    "    cm_model.train() # set model in train model for batchnorm and dropout layers in bert model\n",
    "    pt_model.train()\n",
    "    \n",
    "    #print(cm_model)\n",
    "    #print(pt_model)\n",
    "    \n",
    "    for cm_batch,pt_batch in zip(cm_data_loader,pt_data_loader):\n",
    "        b_cm_input_ids = cm_batch[0].to(device)\n",
    "        b_cm_input_mask = cm_batch[1].to(device)\n",
    "        b_cm_labels = cm_batch[2].to(device)\n",
    "\n",
    "        b_pt_input_ids = pt_batch[0].to(device)\n",
    "        print(b_pt_input_ids.shape)\n",
    "        b_pt_input_mask = pt_batch[1].to(device)\n",
    "        b_pt_labels = pt_batch[2].to(device)\n",
    "        print(b_pt_labels.shape)\n",
    "        \n",
    "        cm_model.zero_grad()\n",
    "        pt_model.zero_grad()\n",
    "            \n",
    "        cm_output = cm_model(b_cm_input_ids,\n",
    "                            attention_mask=b_cm_input_mask,\n",
    "                            labels=b_cm_labels.long())\n",
    "        \n",
    "        cm_loss = cm_output[0]\n",
    "        cm_logits = cm_output[1]\n",
    "\n",
    "        print(cm_loss.item())\n",
    "\n",
    "\n",
    "        pt_output = pt_model(b_pt_input_ids,\n",
    "                            attention_mask=b_pt_input_mask,\n",
    "                            labels=b_pt_labels.long())\n",
    "        \n",
    "        pt_loss = pt_output[0]\n",
    "        pt_logits = pt_output[1]\n",
    "\n",
    "        print(pt_loss.item())\n",
    "            \n",
    "        loss=(1-alpha)*cm_loss.item()+alpha*pt_loss.item()\n",
    "\n",
    "        total_train_loss+=loss\n",
    "\n",
    "        logits = fc_linear(cm_logits,pt_logits)\n",
    "\n",
    "        total_train_acc+=categorical_accuracy(logits,b_pt_labels).item()\n",
    "            \n",
    "        loss.backward()\n",
    "            \n",
    "        torch.nn.utils.clip_grad_norm_(cm_model.parameters(),1.0)\n",
    "        torch.nn.utils.clip_grad_norm_(pt_model.parameters(),1.0)\n",
    "            \n",
    "        optimizer.step()\n",
    "            \n",
    "        scheduler.step() #go ahead and update the learning rate\n",
    "            \n",
    "    avg_train_loss = total_train_loss/len(data_loader)\n",
    "    avg_train_acc = total_train_acc/len(data_loader)\n",
    "    \n",
    "    return avg_train_loss,avg_train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "informed-territory",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 134])\n",
      "torch.Size([32])\n",
      "0.7474662065505981\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-51-7c49c66eec7b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcm_model\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_data_loader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpt_bert_model\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpt_train_data_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-50-9ca35b4a2b5b>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(cm_model, cm_data_loader, pt_model, pt_data_loader)\u001b[0m\n\u001b[0;32m     39\u001b[0m         pt_output = pt_model(b_pt_input_ids,\n\u001b[0;32m     40\u001b[0m                             \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mb_pt_input_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m                             labels=b_pt_labels.long())\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[0mpt_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpt_output\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\py3_env\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\py3_env\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1495\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1496\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1497\u001b[1;33m             \u001b[0mreturn_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1498\u001b[0m         )\n\u001b[0;32m   1499\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\py3_env\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\py3_env\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    954\u001b[0m             \u001b[0mtoken_type_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    955\u001b[0m             \u001b[0minputs_embeds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 956\u001b[1;33m             \u001b[0mpast_key_values_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    957\u001b[0m         )\n\u001b[0;32m    958\u001b[0m         encoder_outputs = self.encoder(\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\py3_env\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\py3_env\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 200\u001b[1;33m             \u001b[0minputs_embeds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_embeddings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    201\u001b[0m         \u001b[0mtoken_type_embeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoken_type_embeddings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\py3_env\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\py3_env\\lib\\site-packages\\torch\\nn\\modules\\sparse.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m         return F.embedding(\n\u001b[0;32m    125\u001b[0m             \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 126\u001b[1;33m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[0;32m    127\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\py3_env\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   1850\u001b[0m         \u001b[1;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1851\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1852\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1853\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1854\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "train(cm_model,train_data_loader,pt_bert_model,pt_train_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sticky-acceptance",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3_env",
   "language": "python",
   "name": "py3_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
